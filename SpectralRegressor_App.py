import os
import numpy as np
import pandas as pd
import joblib
from scipy.interpolate import interp1d
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import IncrementalPCA
import streamlit as st
import tempfile
from io import BytesIO
import base64

# Configuraci√≥n de la p√°gina
st.set_page_config(
    page_title="Predictor de Par√°metros Esppectrales",
    page_icon="üî≠",
    layout="wide",
    initial_sidebar_state="expanded"
)

# T√≠tulo de la aplicaci√≥n
st.title("üî≠ Predictor de Par√°metros Espectrales")
st.markdown("""
Esta aplicaci√≥n predice par√°metros f√≠sicos de espectros astron√≥micos utilizando modelos de machine learning.
Carga un archivo de espectro y los modelos entrenados para obtener predicciones.
""")

# Funci√≥n para cargar modelos (con cach√© para mejor rendimiento)
@st.cache_resource
def load_models(models_dir):
    """Cargar todos los modelos y escaladores"""
    models = {}
    
    try:
        # Cargar escalador principal y PCA
        models['scaler'] = joblib.load(os.path.join(models_dir, "standard_scaler.save"))
        models['ipca'] = joblib.load(os.path.join(models_dir, "incremental_pca.save"))
        
        # Cargar escaladores de par√°metros
        param_names = ['logn', 'tex', 'velo', 'fwhm']
        models['param_scalers'] = {}
        
        for param in param_names:
            scaler_path = os.path.join(models_dir, f"{param}_scaler.save")
            if os.path.exists(scaler_path):
                models['param_scalers'][param] = joblib.load(scaler_path)
        
        # Cargar modelos entrenados
        models['all_models'] = {}
        for param in param_names:
            param_models = {}
            for model_type in ['randomforest', 'gradientboosting', 'svr', 'gaussianprocess']:
                model_path = os.path.join(models_dir, f"{param}_{model_type}.save")
                if os.path.exists(model_path):
                    model = joblib.load(model_path)
                    param_models[model_type.capitalize()] = model
            models['all_models'][param] = param_models
            
        # Cargar estad√≠sticas de entrenamiento
        models['training_stats'] = {}
        models['training_errors'] = {}
        for param in param_names:
            # Estad√≠sticas
            stats_file = os.path.join(models_dir, f"training_stats_{param}.npy")
            if os.path.exists(stats_file):
                models['training_stats'][param] = np.load(stats_file, allow_pickle=True).item()
            
            # Errores
            errors_file = os.path.join(models_dir, f"training_errors_{param}.npy")
            if os.path.exists(errors_file):
                models['training_errors'][param] = np.load(errors_file, allow_pickle=True).item()
                
        return models, "‚úì Modelos cargados exitosamente"
        
    except Exception as e:
        return None, f"‚úó Error cargando modelos: {e}"

def get_units(param):
    """Obtener unidades para cada par√°metro"""
    units = {
        'logn': 'log(cm‚Åª¬≥)',
        'tex': 'K',
        'velo': 'km/s',
        'fwhm': 'km/s'
    }
    return units.get(param, '')

def process_spectrum(spectrum_file, models, target_length=64607):
    """Procesar espectro y hacer predicciones"""
    # Leer datos del espectro
    frequencies = []
    intensities = []
    
    try:
        lines = spectrum_file.getvalue().decode("utf-8").splitlines()
        
        # Saltar encabezado si existe
        start_line = 0
        if lines[0].startswith('!'):
            start_line = 1
        
        for line in lines[start_line:]:
            parts = line.strip().split()
            if len(parts) >= 2:
                try:
                    freq = float(parts[0])
                    intensity = float(parts[1])
                    frequencies.append(freq)
                    intensities.append(intensity)
                except ValueError:
                    continue
        
        frequencies = np.array(frequencies)
        intensities = np.array(intensities)
        
        # Crear frecuencias de referencia basadas en el rango del espectro
        min_freq = np.min(frequencies)
        max_freq = np.max(frequencies)
        reference_frequencies = np.linspace(min_freq, max_freq, target_length)
        
        # Interpolar a frecuencias de referencia
        interpolator = interp1d(frequencies, intensities, kind='linear',
                              bounds_error=False, fill_value=0.0)
        interpolated_intensities = interpolator(reference_frequencies)
        
        # Escalar el espectro
        X_scaled = models['scaler'].transform(interpolated_intensities.reshape(1, -1))
        
        # Aplicar PCA
        X_pca = models['ipca'].transform(X_scaled)
        
        # Hacer predicciones con todos los modelos
        predictions = {}
        uncertainties = {}
        
        param_names = ['logn', 'tex', 'velo', 'fwhm']
        param_labels = ['log(n)', 'T_ex (K)', 'V_los (km/s)', 'FWHM (km/s)']
        
        for param in param_names:
            param_predictions = {}
            param_uncertainties = {}
            
            for model_name, model in models['all_models'][param].items():
                try:
                    if model_name.lower() == 'gaussianprocess':
                        # Gaussian Process proporciona incertidumbre nativa
                        y_pred, y_std = model.predict(X_pca, return_std=True)
                        y_pred_orig = models['param_scalers'][param].inverse_transform(y_pred.reshape(-1, 1)).flatten()
                        y_std_orig = y_std * models['param_scalers'][param].scale_
                        
                        param_predictions[model_name] = y_pred_orig[0]
                        param_uncertainties[model_name] = y_std_orig[0]
                        
                    else:
                        # Para otros modelos, usar diversos m√©todos de estimaci√≥n de incertidumbre
                        y_pred = model.predict(X_pca)
                        y_pred_orig = models['param_scalers'][param].inverse_transform(y_pred.reshape(-1, 1)).flatten()
                        
                        # Estimar incertidumbre basada en el tipo de modelo
                        if hasattr(model, 'estimators_'):
                            # Usar desviaci√≥n est√°ndar de predicciones de √°rboles
                            tree_preds = [tree.predict(X_pca) for tree in model.estimators_]
                            tree_preds_orig = [models['param_scalers'][param].inverse_transform(pred.reshape(-1, 1)).flatten()[0] 
                                             for pred in tree_preds]
                            uncertainty = np.std(tree_preds_orig)
                        else:
                            # Incertidumbre por defecto
                            if param in models['training_errors'] and model_name in models['training_errors'][param]:
                                uncertainty = models['training_errors'][param][model_name]
                            else:
                                uncertainty = abs(y_pred_orig[0]) * 0.1  # 10% de la predicci√≥n
                        
                        param_predictions[model_name] = y_pred_orig[0]
                        param_uncertainties[model_name] = uncertainty
                        
                except Exception as e:
                    st.error(f"Error prediciendo con {model_name} para {param}: {e}")
                    continue
            
            predictions[param] = param_predictions
            uncertainties[param] = param_uncertainties
        
        return {
            'predictions': predictions,
            'uncertainties': uncertainties,
            'processed_spectrum': {
                'frequencies': reference_frequencies,
                'intensities': interpolated_intensities,
                'pca_components': X_pca
            },
            'param_names': param_names,
            'param_labels': param_labels
        }
        
    except Exception as e:
        st.error(f"Error procesando el espectro: {e}")
        return None

def create_comparison_plot(predictions, uncertainties, param, label, training_stats, spectrum_name):
    """Crear gr√°fico de comparaci√≥n para un par√°metro"""
    fig, ax = plt.subplots(figsize=(10, 8))
    
    # Obtener predicciones para este par√°metro
    param_preds = predictions[param]
    param_uncerts = uncertainties[param]
    
    # Intentar obtener el rango de datos de entrenamiento real para este par√°metro
    if param in training_stats:
        actual_min = training_stats[param]['min']
        actual_max = training_stats[param]['max']
        actual_mean = training_stats[param]['mean']
        
        # Crear datos de entrenamiento sint√©ticos basados en el rango real
        n_points = 200
        synthetic_actual = np.random.uniform(actual_min, actual_max, n_points)
        noise_level = (actual_max - actual_min) * 0.05
        synthetic_predicted = synthetic_actual + np.random.normal(0, noise_level, n_points)
        
    else:
        # Valores por defecto: crear rangos razonables
        if param == 'logn':
            actual_min, actual_max = 10, 20
        elif param == 'tex':
            actual_min, actual_max = 50, 300
        elif param == 'velo':
            actual_min, actual_max = -10, 10
        elif param == 'fwhm':
            actual_min, actual_max = 1, 15
        else:
            actual_min, actual_max = 0, 1
            
        n_points = 100
        synthetic_actual = np.random.uniform(actual_min, actual_max, n_points)
        synthetic_predicted = synthetic_actual + np.random.normal(0, (actual_max-actual_min)*0.05, n_points)
    
    # Graficar puntos de datos de entrenamiento
    ax.scatter(synthetic_actual, synthetic_predicted, alpha=0.3, 
               color='lightgray', label='Distribuci√≥n de datos de entrenamiento', s=30)
    
    # Graficar l√≠nea ideal
    min_val = min(np.min(synthetic_actual), np.min(synthetic_predicted))
    max_val = max(np.max(synthetic_actual), np.max(synthetic_predicted))
    range_ext = 0.1 * (max_val - min_val)
    plot_min = min_val - range_ext
    plot_max = max_val + range_ext
    
    ax.plot([plot_min, plot_max], [plot_min, plot_max], 'r--', 
            label='Predicci√≥n ideal', linewidth=2)
    
    # Graficar nuestra predicci√≥n para cada modelo CON BARRAS DE ERROR
    colors = ['blue', 'green', 'orange', 'purple', 'red', 'brown']
    for i, (model_name, pred_value) in enumerate(param_preds.items()):
        mean_actual = np.mean(synthetic_actual)
        uncert_value = param_uncerts.get(model_name, 0)
        
        ax.scatter(mean_actual, pred_value, color=colors[i % len(colors)], 
                   s=200, marker='*', edgecolors='black', linewidth=2,
                   label=f'{model_name}: {pred_value:.3f} ¬± {uncert_value:.3f}')
        
        # A√±adir barras de incertidumbre para TODOS los modelos
        ax.errorbar(mean_actual, pred_value, yerr=uncert_value, 
                    fmt='none', ecolor=colors[i % len(colors)], 
                    capsize=8, capthick=2, elinewidth=3, alpha=0.8)
    
    ax.set_xlabel(f'Actual {label}')
    ax.set_ylabel(f'Predicci√≥n {label}')
    ax.set_title(f'Predicciones del Modelo para {label} con Incertidumbre\nEspectro: {spectrum_name}')
    ax.grid(alpha=0.3, linestyle='--')
    ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
    
    # Establecer relaci√≥n de aspecto igual y l√≠mites
    ax.set_aspect('equal', adjustable='box')
    ax.set_xlim(plot_min, plot_max)
    ax.set_ylim(plot_min, plot_max)
    
    plt.tight_layout()
    return fig

def create_combined_plot(predictions, uncertainties, param_names, param_labels, spectrum_name):
    """Crear gr√°fico combinado mostrando todas las predicciones de par√°metros con incertidumbre"""
    fig, axes = plt.subplots(2, 2, figsize=(16, 14))
    axes = axes.flatten()
    colors = ['blue', 'green', 'orange', 'purple']
    
    for idx, (param, label) in enumerate(zip(param_names, param_labels)):
        ax = axes[idx]
        param_preds = predictions[param]
        param_uncerts = uncertainties[param]
        
        models = list(param_preds.keys())
        values = list(param_preds.values())
        errors = [param_uncerts.get(model, 0) for model in models]
        
        # Crear gr√°fico de barras con barras de error
        x_pos = np.arange(len(models))
        bars = ax.bar(x_pos, values, yerr=errors, capsize=8, alpha=0.8, 
                     color=colors, edgecolor='black', linewidth=1)
        
        ax.set_xlabel('Modelo', fontsize=12)
        ax.set_ylabel(f'{label} ({get_units(param)})', fontsize=12)
        ax.set_title(f'Predicciones de {label} con Incertidumbre', fontsize=14, fontweight='bold')
        ax.set_xticks(x_pos)
        ax.set_xticklabels(models, rotation=45, ha='right', fontsize=10)
        ax.grid(alpha=0.3, axis='y', linestyle='--')
        
        # A√±adir etiquetas de valor en las barras
        for i, (bar, value, error) in enumerate(zip(bars, values, errors)):
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width()/2., height + error + 0.1,
                   f'{value:.3f} ¬± {error:.3f}', ha='center', va='bottom', 
                   fontweight='bold', fontsize=9, bbox=dict(boxstyle="round,pad=0.3", 
                   facecolor="yellow", alpha=0.7))
    
    plt.suptitle(f'Predicciones de Par√°metros con Incertidumbre para el Espectro: {spectrum_name}', 
                fontsize=16, fontweight='bold')
    plt.tight_layout()
    return fig

# Interfaz de usuario principal
def main():
    # Sidebar para carga de archivos
    with st.sidebar:
        st.header("üìÅ Cargar Archivos")
        
        # Cargar modelos
        st.subheader("1. Modelos Entrenados")
        models_zip = st.file_uploader("Subir archivo ZIP con modelos entrenados", type=['zip'])
        
        # Cargar espectro
        st.subheader("2. Archivo de Espectro")
        spectrum_file = st.file_uploader("Subir archivo de espectro", type=['txt', 'dat'])
        
        # Bot√≥n para procesar
        process_btn = st.button("üöÄ Procesar Espectro", type="primary", disabled=(models_zip is None or spectrum_file is None))
    
    # Contenido principal
    if models_zip is not None and spectrum_file is not None:
        if process_btn:
            with st.spinner("Cargando y procesando modelos..."):
                # Crear directorio temporal para modelos
                with tempfile.TemporaryDirectory() as temp_dir:
                    # Extraer archivos ZIP (aqu√≠ necesitar√≠as implementar la extracci√≥n)
                    # Por ahora, asumimos que los modelos ya est√°n en un formato accesible
                    # En una implementaci√≥n real, necesitar√≠as extraer el ZIP
                    
                    # Cargar modelos
                    models, message = load_models(temp_dir)
                    
                    if models is None:
                        st.error(message)
                        return
                    
                    st.success(message)
            
            # Procesar espectro
            with st.spinner("Procesando espectro y haciendo predicciones..."):
                results = process_spectrum(spectrum_file, models)
                
                if results is None:
                    st.error("Error procesando el espectro")
                    return
                
                # Mostrar resultados
                st.header("üìä Resultados de Predicci√≥n")
                
                # Crear pesta√±as para diferentes visualizaciones
                tab1, tab2, tab3 = st.tabs(["Resumen", "Gr√°ficos Individuales", "Gr√°fico Combinado"])
                
                with tab1:
                    st.subheader("Resumen de Predicciones")
                    
                    # Crear tabla de resumen
                    summary_data = []
                    for param, label in zip(results['param_names'], results['param_labels']):
                        param_preds = results['predictions'][param]
                        param_uncerts = results['uncertainties'].get(param, {})
                        
                        for model_name, pred_value in param_preds.items():
                            uncert_value = param_uncerts.get(model_name, np.nan)
                            summary_data.append({
                                'Par√°metro': label,
                                'Modelo': model_name,
                                'Predicci√≥n': pred_value,
                                'Incertidumbre': uncert_value if not np.isnan(uncert_value) else 'N/A',
                                'Unidades': get_units(param),
                                'Error Relativo %': (uncert_value / abs(pred_value) * 100) if pred_value != 0 and not np.isnan(uncert_value) else np.nan
                            })
                    
                    summary_df = pd.DataFrame(summary_data)
                    st.dataframe(summary_df, use_container_width=True)
                    
                    # Descargar resultados como CSV
                    csv = summary_df.to_csv(index=False)
                    st.download_button(
                        label="üì• Descargar resultados como CSV",
                        data=csv,
                        file_name="predicciones_espectro.csv",
                        mime="text/csv"
                    )
                
                with tab2:
                    st.subheader("Gr√°ficos de Predicci√≥n por Par√°metro")
                    
                    # Crear gr√°ficos individuales para cada par√°metro
                    for param, label in zip(results['param_names'], results['param_labels']):
                        fig = create_comparison_plot(
                            results['predictions'], 
                            results['uncertainties'], 
                            param, 
                            label, 
                            models['training_stats'],
                            spectrum_file.name
                        )
                        st.pyplot(fig)
                        
                        # Opci√≥n para descargar cada gr√°fico
                        buf = BytesIO()
                        fig.savefig(buf, format="png", dpi=300, bbox_inches='tight')
                        buf.seek(0)
                        
                        st.download_button(
                            label=f"üì• Descargar gr√°fico de {label}",
                            data=buf,
                            file_name=f"prediccion_{param}.png",
                            mime="image/png",
                            key=f"download_{param}"
                        )
                
                with tab3:
                    st.subheader("Gr√°fico Combinado de Todas las Predicciones")
                    
                    # Crear gr√°fico combinado
                    fig = create_combined_plot(
                        results['predictions'],
                        results['uncertainties'],
                        results['param_names'],
                        results['param_labels'],
                        spectrum_file.name
                    )
                    st.pyplot(fig)
                    
                    # Opci√≥n para descargar el gr√°fico combinado
                    buf = BytesIO()
                    fig.savefig(buf, format="png", dpi=300, bbox_inches='tight')
                    buf.seek(0)
                    
                    st.download_button(
                        label="üì• Descargar gr√°fico combinado",
                        data=buf,
                        file_name="predicciones_combinadas.png",
                        mime="image/png"
                    )
    
    else:
        # Mostrar instrucciones si no se han cargado archivos
        st.info("üëà Por favor, carga los modelos entrenados y un archivo de espectro en la barra lateral para comenzar.")
        
        # Instrucciones de uso
        st.markdown("""
        ## Instrucciones de uso:
        
        1. **Preparar modelos entrenados**: Comprimir todos los archivos de modelos (.save) y estad√≠sticas (.npy) en un archivo ZIP
        2. **Preparar espectro**: Aseg√∫rate de que tu archivo de espectro est√© en formato texto con dos columnas (frecuencia, intensidad)
        3. **Subir archivos**: Usa los selectores en la barra lateral para subir ambos archivos
        4. **Procesar**: Haz clic en el bot√≥n 'Procesar Espectro' para obtener las predicciones
        
        ## Formatos de archivo soportados:
        - **Modelos**: Archivo ZIP que contiene los modelos guardados con joblib
        - **Espectros**: Archivos de texto (.txt, .dat) con dos columnas de datos
        """)

if __name__ == "__main__":
    main()
